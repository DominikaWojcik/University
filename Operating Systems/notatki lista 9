ZAD 1:

Dzielimy dysk na partycje po to by:
a) Różne systemy operacyjne mogły współistnieć
b) Żeby podzielić dysk na części kategoriami (system i programy), (dane użytkownika), (dzienniki i cache) izolacja, by móc łatwiej robić kopie zapasowe
c) Trzymać partycję na stronicowanie - wymiane stron

MBR master block record - pierwszy sektor na dysku 512 bajtów zawiera 446 (?) bajtów na kod rozruchowy czytany przez BIOS

potem mamy 4 16bajtowych wpisów na główne partycje - każdy wpis definiuje, gdzie się zaczyna i gdzie się konczy partycja, zawiera typ partycji etc.

Początek i koniec jest zapisywany na dwa sposoby:
1) format cylinder/head/sector - 24 bitowy (3 bajty)
2) LBA - logical block adressing - 32 bitowy (4 bajty)

typ partycji - 1 bajt - 256 kodów oznaczających rodzaj partycji. np. FAT16, NTFS, linux swap, ext

flagi: czy partycja jest bootowalna

Mogą być tylko 4 partycje gówne, jeśli chcę mieć jakieś inne, to mogę poświęcić jedno miejsce na partycję głowną na n logicznych partycji o jakimś ustalonym rozmiarze

Możemy adresować zaledwie 0,5KB * 2^32 (4 bajty) = 2TB
Dlatego większe dyski nie mogą mieć MBR

Jeśli ktoś nam zepsuje coś w MBR, to rip nie ma jak tego naprawic.

GPT - GUID Partition Table

Składa się z ochronnego sektoru MBR, nagłówka tablicy partycji oraz z tablicy partycji ( do 128 wpisów).

Nie ma starej notacji początków i końców bloków CHS, używamy tylko LBA, do adresów używamy 8 bajtów, więc możemy obsługiwać dyski 0,5KB * 2^64 =? 8.6 * 1-^9 * TB

Struktury danych GPT są przechowywane dwa razy na dysku: raz na końcu, raz na początku. Dla struktur GPT oblicza się cyclic redundancy check - sumy kontrolne - hashuje się zawartośc sektora.

Nie ma potrzeby rozszerzonych (logicznych) partycji, bo mamy miejsce aż na 128 partycji.

Typ partycji kodowany aż na 16 bajtach. 

Przechowywana jest nazwa partycji czytelna dla człowieka.

Ponieważ nie wszystkie narzędzia operujące na dyskach muszą obsługiwać GPT, umieszcza się na początku ochronne MBR o rozmiarze max(2TB, rozmiar dysku) po to, by nieświadome istnienia GPT narzędzia zobaczyły, że cały dysk jest zajęty przez partycję o typie EFI (0xEE).

Przez to starsze systemy (sprzed windows vista) mogą też być instalowane na dysku.

Przez to, że mamy MBR, komputery używające BIOSa mogą uruchamiać komputer za pomocą kodu rozruchowego MBR, ale tylko jeśli bootloader jest świadomy istnienia GPT


ZAD2: 

Wirtualne urządzenia blokowe - odnosimy się z przestrzeni użytkownika do urządzenia /dev/mapper - lecz wszystkie operacje na nim przechodzą przez moduł device mapper, który tłumaczy i odsyła żadania do fizycznych urządzeń już w przestrzeni jadra.

Mamy warstwy translacji - metoda tłumaczenia żądań coś w ten deseń: przykładowe warstwy translacji:

-linear - przydzielamy z jakiegoś urządzenia blokowego ciągły kawałek pamięci

-striped - przydzielamy z n urządzeń naprzemiennie chunki pamięci np. 32KB |a|b|a|b|a|b|a|b

-cache - wybieramy dwa (trzy, bo jeszcze jedno jest potrzebne na metadane) urządzenia blokowe, jedno (małe) posłuży jako pamięć podręczna, a drugie (duże) jako magazyn danych.

-crypt - szyfrujemy urządzenie blokowe - wszystkie operacje są szyfrowane

-mirror - dwa lub więcej urządzenia "współdzielą" dane. Dane są kopiowane między urządzeniami w czasie rzeczywistym.

By z dwóch 2TB HDD i 32GB SSD zrobic szybkie i niezawodne urządzenie 2TB, musimy połączyć najpierw w jedno urządzenie warstwą translacji mirror dwa duże dyski. Następnie stosujemy na uprzednio stworzonym urządzeniu i dysku SSD warstwę translacji cache.

By z dwóch 1TB HDD stworzyć 2TB zaszyfrowane urządzenie blokowe, należy translacją striped połączyć dyski 1TB ze sobą w jeden duży 2TB (Ze względu na szybkość wybieramy striped zamiast linear). Następnie należy nałożyć warstwę translacji crypt, by operacje na stworzonym dużym dysku były szyfrowane.

RAID: Redundant array of independent disks

Raid jest sprzętowym mechanizmem używającym kilku dysków w sposób wyspecyfikowany przez poziom. Poziom to po prostu typ. W praktyce używa się tylko poziomów 0, 1, 5 i 6.

RAID level 0: po prostu bierzemy dyski i stripujemy je.

RAID level 1: stripujemy dyski, ale drugie tyle dysków używamy do przechowywania tych samych danych. Żądania są rozdzielane do jednego z dwóch dysków przechowywujących te same dane. Zapisy muszą być wykonywane równolegle.

RAID level 2: dyski zestripowane (po co? nie wiem) i głowice dysków są ze sobą zsynchronizowane. Na zbędnych dyskach w odpowiednich miejsach zapisuje się kod poprawiający błędy. Kod ten stworzony jest dla bitów na tych samych pozycjach w różnych dyskach. Stripy są bardzo małe 1 bajt albo słowo. Używane są kody Hamminga do naprawy błędu jednego bitu i do wykrywania błędów 2 bitów.


RAID level 3: to samo, co 2, ale zapamiętujemy tylko bit parzystości dla każdego bitu na tym samym miejscu. Czyli xor wartości bitów.

x4 = x0 + x1 + x2 + x3 (+ to xor)
Jeśli x1 sie zepsuł, to
w takim wyapdku obustronnie xorujac przez (x4 + x1)
x1 = x0 + x2 + x3 + x4

Musimy iec tylko n+1 dysków.

Raid level4:

Dzielimy dyski na stripe'y. Każdy dysk działa niezależnie. Utrzymujemy dysk na bity parzystości dla każdego bitu zgrupowanych ze sobą pasków.
x4 = x0 + x1 + x2 + x3
x4' = x0 + x1' + x2 + x3
x4' = x4 + x1 + x1'
Za każdym razem, gdy coś modyfikujemy (robimy zapis), musimy porównać stare wartości i nowe wartości. jest to narzut czasowy.

N+1 dysków.

Raid level 5: to samo, co 4, ale paski parzystości znajdują się na różnych dyskach. Np. w kolejności 1,2,3... aż do n. I tak w kółko. Jeśli stracę dysk, to nie tracę wszystkiego.
n+1 dysków.

Raid level 6: 
N+2 dysków. Robimy to samo, ale obliczamy bity parzystości stripów w dwa różne sposoby, i paski parzystości dla tej samej grupy stripów, nie mogą być na tym samym dyski.

ZAD 3:
Szeregowanie ruchów głowicy dysku: takie szeregowanie obrotów, by uzyskać sumaryczny optymalny czas.
!! Oficjalne info Okazuje się, że szeregowanie ruchów głowicy dysku oblicza dysk twardy (jest to poziom niżej)
Natomiast planowanie I/O liczy system operacyjny.

Planowanie wejscia/wyjscia - podejmowanie takich decyzji, by osiągnąć jakiś cel np. zminimalizować czas marnowany przez obroty dysku twardego, priorytetyzować wejscie wyjscie jakiegoś procesu, uczciwie rozdawać czas i/o procesom, spełniać w żądania w zadanym czasie.

noop scheduler: kolejka FIFO z łączeniem żądań, jeśli dotyczą sąsiednich sektorów. Dobre, jeśli jesteśmy CPU bound, a dysk jest wystarczająco szybki - np SSD Trzeba wiedzieć, które sektory na dysku czytają / modyfikują

Dobre gdy system operacyjny nie może przewidzieć z góry, które bloki gdzie tak naprawdę będziemy używać. Np. gdy mamy RAID i kilka dysków.

deadline scheduler: mamy kolejkę elevator scheduler (posortowana kolejka z łączeniem żądań jeśli są sąsiednie) i kolejki fifo na read i write operacje. Każde żądanie ma termin ważności (normalnie read 500ms, write 5 seconds).

Normalnie kolejna operacja jest brana z elevator queue, ale jeśli któraś operacja z początku którejś z kolejek fifo przeterminowała się, to obsługujemy ją i parę kolejnych operacji z tej samej kolejki. Zapobiega to głodzeniu i rozwiązuje problem czytania i pisania (zazwyczaj program piszący pisze i kontynuuje kod, a czytający czyta i czeka na dane).

Ponoć dobre dla mocno wielowątkowych systemów (??? wiki) i przy bazach danych.

anticipatory scheduler: Poprzedni planiści próbowali spełniać żądania zawsze po zakończeniu jednego. Często zdarza się, że po operacji czytania, program znowu będzie czytał. Z dużym prawdopodobieństwem, adres pod którym będzie czytał będzie w pobliżu tego, gdzie skończyliśmy ostatnie czytanie.
Jeśli po spełnieniu operacji poczekamy i dostaniemy żądanie operacji w pobliżu, to drastycznie zaoszczędzimy czas.
Anticipatory scheduler to po prostu deadline scheduler z dołączonym czekaniem na kolejne żądanie. Na linuxie czeka się do 6ms

completely fair queuing: Dla synchronicznych żądań stworzone są kolejki (1 na proces) ponieważ każdy proces ma różny priorytet, to każdy proces dostaje swój kwant czasu w zależności od priorytetu.

Dla asynchronicznych żądań mamy kolejki po jednej dla każdego priorytetu.

Dla procesu, jeśli okaże się, że po wykonaniu swoich operacji w kwancie czasu, to na podstawie statystyk, można tak samo jak w przypadku anticipatory schedulera poczekać trochę na kolejne operacje, które prawdopodobnie będą w pobliżu ostatnich.

Dobre dla dysków, których seek time jest długi. Słabe dla SSD. 
Opóźnienia mogą napsuć trochę krwi - np. gdy mamy RAID, to czekanie może się okazać bez sensu. Albo nawet RAID może sam mieć planistę, który czeka.
Dobre, gdy mamy pod ręką dużo danych, ale nie jeśli do tego dodamy wiele non work conserving planistów (czyli takich, co czekają), bo ci na niższych poziomach będą dostawać nieadekwantne dane. 

Okazuje się, że CFQ ma całkiem dobra przepustowość.

ZAD 4:
Top half and bottom half przerwań.

Przerwania muszą być szybkie i wykonywać często dużo pracy - sprzeczność.
Przykład: Karta sieciowa - cały czas dostajemy pakiety.
Kiedy dostajemy przerwanie, musimy powiedzieć urządzeniu, że zrozumiano przerwanie. Skopiować do pamięci głównej dane, które odebraliśmy i przygotowuje kartę sieciową na odbieranie kolejnych danych.
Ponieważ czas nas goni, to nie chcemy przetwarzać pakietu już teraz. Jeśli będziemy zwlekać, to zmniejszymy przepustowość. Bufory na pakiety mogą się przepełnić - stracimy dane. 

Jeszcze niektóre przerwania wyłączają inne przerwania na czas działania procedury obsługi. Jeśli ten czas będzie długi, to możemy mieć kłopoty.

Zatem lepiej od razu szybko wykonać, to co jest konieczne: rozpoznanie, ze nasze urządzenie zgłosiło przerwanie, odebranie danych i przygotowanie urządzenia do dalszego odbierania danych. - to jest górna połowa przerwania.

Dolna połowa przerwania to wszystko, co możemy zostawić sobie na jakiś inny moment, kiedy będziemy mieć czas.

Jakie ograniczenia na górną połowe:
a) Jeśli praca jest wrażliwa na czas, to trzeba ją wykonać w górnej połowie.
b) Jeśli praca do wykonania jest powiązana ze sprzętem, to trzeba ją wykonać w górnej połowie.
c) Jeśli pracaca musi wymusić, żeby żadne inne przerwanie (w szczególności z tego samego urządzenia) jej nie przerwało, to trzeba ją wykonać w górnej połowie.

Na wszystko inne mamy wyrąbane, można odłożyć na potem.


Jak górna połowa komunikuje się z dolną? Górna połowa wykonuje prace, którą musi koniecznie zrobić, a potem wrzuca pozostałą pracę do jakiejś struktury danych:

Jak zaimplementować dolne połowy?

a) BH - oryginalny bottom half. Były 32 dolne połowy, jeśli chcialo się użyć jednej, to ustawiało się specjalny bit na 1. Dolne połowy zawsze działały rozłącznie. Troche bottleneck

b) task queues - przechowywano zbiór kolejek. W każdej kolejce były funkcje do wywołania. Funkcje te były wywoływane w zależności od kolejki w jakimś czasie. Sterowniki rejestrowały swoje funkcje do odpowiednich kolejek. Ponoć zbyt ciężkie jak dla networkingu.

c) work queue - mamy kolejkę, na której operuje wątek jądra. Dobre, jeśli chcemy zarządzać (planować) dolnymi połówkami.
KWORKERZY z warsztatów - to oni odwalają robotę

d) softirq - statycznie zdefiniowana dolna połówka, mogą działać w tym samym momencie na różnych procesorach, ale dwa takie same nie. Dwa takie same mogą co najwyżej współbieżnie ale nie jednocześnie.

Dla softirq'ów zaznacza się, że mają zostać wywołane. Potem jądro sobie je wywoła w odpowiednim czasie: ksoftirqd kernel thread.

e) tasklety - dynamicznie tworzone dolne połówki. Zbudowane na podstawie softirqów. Podobnie jak z softirq'ami różne mogą działać jednocześnie, ale takie same nie mogą. Tasklety mogą mieć różne priorytety.
Ponieważ tasklety są softirq'ami, są także obsługiwane przez te same wątki jądra ksoftirqd w swoim czasie.

Softirqi lepsze od taskletów, kiedy wydajność jest ważna.



Co jeśli przerwania dla różnych urządzeń używają tych samych numerów? Jądro dla każdej linii przechowuje zarejestrowane procedury obsługi. Kiedy dostaje interrupt pod jakimś numerem, po kolei odpala wszystkie zarejestrowane handlery. Wtedy procedura obsługi musi szybko sprawdzić, czy jej urządzenie faktycznie wysłało przerwanie. Często ustawiony jest interrupt bit urządzenia.

ZAD 6:
Program ładujący łąduje obraz jądra systemu operacyjnego do pamięci.

Ponieważ chcemy, by bootloader miał dużo funkcji, będzie zajmował dużo miejsca. Ale nie zawsze tego miejsca nam styka, więc trzeba podzielić kod bootloadera na części i po kolei je wczytywać.

Obrazy jądra przechowywane są gdzieś na dysku w jakichś systemach plików. Z poziomu biosa nie mamy zielonego pojęcia o czymś takim jak systemy plików, ale jakos trzeba znaleźć plik.

2 opcje:
a) słaba - zrobić mapowania, gdzie obrazy jądra się znajdują, ale to trzeba zawsze aktualizować...
b) zrobić bootloader świadomy istnienia systemów plików. Należy wczytać sterowniki systemów plików.
Następnie można już wczytać konfiguracje bootloadera, która znajduje się też na jakimś systemie plików.

GRUB implementuje opcje b (wadą jest to, że trzeba zużywać więcej miejsca, oraz kod jest bardziej skomplikowany)

Mamy 3 fazy:
Faza 1: Prosto z sektora rozruchowego MBR wczytujemy kod boot.img. Kod ten jest skonfigurowany by wczytać pierwszy sektor core.img (kolejna faza)

Faza 1.5: Wczytujemy core.img, teraz core.img wczytuje sterowniki systemów plików. (generowane z diskboot.img). Następnie wczytuje kolejną fazę używając ścieżki w systemie plików do /boot/grub

Faza 2: pliki w /boot/grub są wczytywane. Odpalany jest interfejs tekstowy do wyboru jądra, można przejść do wiersza polecen i zmienic konfiguracje (e). Lepiej zmienić parametry jądra przed uruchomieniem, ponieważ jeśli jakiś moduł się zepsuje, to nie będziemy tego mogli naprawić i uruchomić systemu. W ten sposób możemy wyłączyć ładowanie jakichs modułów. albo (c) i przejść do grub command line.

Grub umożliwia chain loading : odpala innego bootloader (naprzykład windowsowego, który nie toleruje innych systemów)

Po co ramdysk? Sterowniki dla jądra są zawarte w dynamicznie ładowalnych modułach (gdybysmy statycznie kompilowali te moduły, to zabrałoby to zbyt duzo miejsca). Ale które moduły wczytać i jak, by zamontować głowny system plików?
Musi umieć rozszyfrować partycje zaszyfrowane.
Musi znać strukturę urządzenia (jeśli jest ono wirtualne)

Gorzej, system plikó może być gdzieś ukryty za RAIDem albo zaszyfrowany.

Żeby nie rozpatrywać 10000 przypadków, na samym początku do systemu ładuje się ramdysk z tymczasowym systemem plików. Ten system plików zawiera narzędzia do wykrywania sprzętu, wczytywania modułów oraz wykrywania urządzeń potrzebnych do wczytania prawdziwego systemu plików.

https://en.wikipedia.org/wiki/Initrd

Polegamy na BIOSie/ UEFI, bo sami nie mamy żadnych sterowników, niczego, a jakoś musimy wczytywać odpowiednie dane.

Program ładujący powinien umożliwiać wybór systemu operacyjnego, zmianę konfiguracji, wyłączania właśnie modułów do wczytywania etc.

Musi umieć przeprowadzić uwierzytelnianie: poprosić o hasło.

Linia poleceń jądra to są po prostu boot options. Umożliwia zmiane konfiguracji jądra przed uruchomieniem go. Np. można zablokować wczytywanie (blacklist) jakichś modułów.

boot sector -> lba, len

stage 1,5
+partycje (mało: FAT, EXT 2/3/4)
+File system
+ścieżka do /boot/grub (tego wybranego, może być wiele linuksie na komputerze)

stage 2: /boot/grub
+moduły
+konfiguracja







ZAD 8:

Aby uzyskac dostęp do danych na dysku muszą zostać wykonane następujące operacje:

Głowica dysku musi się przesunąć na odpowiednią ścieżkę - jest to seek time.
Następnie dysk musi się obrócić, by głowica znajdowała się na początku pierwszego sektora do odczytu. To jest rotational delay.

Access time = seek time + rotational delay.

Transfer time = Access time + (liczba bajtów do transferu) / (Liczba bajtó na ścieżce * prędkość obrotowa (Rotacje na sekunde))